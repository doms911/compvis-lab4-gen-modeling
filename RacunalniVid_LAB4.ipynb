{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Predgovor\n",
    "Ova vježba sastoji se od teoretskog i praktičnog dijela. Bez adekvatnog teorijskog razumijevanja, izvedba vježbe biti će značajno izazovnija.\n",
    "\n",
    "Predlažemo da napravite kopiju ove bilježnice na svoj Colab profil i tamo riješite vježbu. Ne zaboravite uključiti GPU akceleraciju (Runtime->Change runtime type->GPU->Save). \\\n",
    "\n",
    "U slučaju nejasnoća javite se na anja.delic@fer.hr."
   ],
   "metadata": {
    "id": "uhIzBADQDNCB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Laboratorijska vježba - generativno modeliranje"
   ],
   "metadata": {
    "id": "vKtPXFrnpYzC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jedan od glavnih katalizatora u razvoju modernog računalnog vida jest dostupnost velikih skupova podataka. Iako su skupovi podataka lako dostupni, proces koji generira podatke je često nepoznat. Cilj ove laboratorijske vježbe jest pokušati aproksimirati proces koji generira dani skup podataka.\n"
   ],
   "metadata": {
    "id": "uzIreOYS7B8T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teorijska podloga generativnom modeliranju"
   ],
   "metadata": {
    "id": "kcPMUvv3E1pL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Krenuti ćemo od bolje definicije zadatka.\n",
    "Pretpostavimo postojanje skupa podataka $D=\\{\\mathbf{x}_i\\}_{i=1}^N$ koji je nastao uzorkovanjem nepoznate distribucije $P_D(\\underline{\\mathbf{x}})$. Nadalje, pretpostavimo da je svaki primjer $\\mathbf{x}_i$ dobiven nezavisnim uzorkovanjem. Naš cilj je aproksimirati nepoznatu distribuciju podataka $P_D(\\underline{\\mathbf{x}})$ poznatim modelom $P_\\theta(\\underline{\\mathbf{x}})$.\n",
    "S obzirom da su naši podaci često u kontinuirani ($\\underline{\\mathbf{x}}$ je kontinuirana slučajna varijabla) baviti ćemo se modeliranjem gustoća vjerojatnosti $p_D(\\underline{\\mathbf{x}})$ i $p_\\theta(\\underline{\\mathbf{x}})$."
   ],
   "metadata": {
    "id": "f9YQxQbbEz7w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 1.:** Kako poravnati nepoznati $p_D(\\underline{\\mathbf{x}})$ i poznati $p_\\theta(\\underline{\\mathbf{x}})$?\n",
    "\n"
   ],
   "metadata": {
    "id": "HHwh1wyJASSH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "U teoriji vjerojatnosti, različitost između dvije distribucije $P(\\underline{x})$ i $Q(\\underline{x})$ se mjeri pomoću f-divergencije. Jedna od najčešćih mjera iz obitelji f-divergencija je KL-divergencija. KL-divergenciju za kontinuirane slučajne varijable definiramo kao:\n",
    "$$\n",
    "\\mathrm{KL}(P, Q) = \\int_{-\\infty}^{+\\infty} p(x) \\ln\\frac{p(x)}{q(x)} dx.\n",
    "$$\n",
    "$p$ i $q$ su pripadajuće gustoće vjerojatnosti."
   ],
   "metadata": {
    "id": "G9Sv47JgCUS-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "KL-divergencije između distribucije podataka i distribucije modela je primamljiv gubitak za naš optimizacijski postupak. Može se pokazati da je minimizacija KL divergencije između distribucije podataka i distribucije modela ekvivalentna minimizaciji negativne log-izglednosti skupa podataka za učenje:\n",
    "$$\n",
    "\\underset{\\theta}{\\mathrm{min}} \\, \\mathrm{KL}(p_D, p_\\theta) \\simeq \\underset{\\theta}{\\mathrm{min}} \\, -\\mathbb{E}_{\\mathbf{x} \\in D} \\;\\;  [\\ln p_\\theta(\\mathbf{x})]\n",
    "$$"
   ],
   "metadata": {
    "id": "ryOKIYqGGd1d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 1.**: Dokažite navedenu tvrdnju."
   ],
   "metadata": {
    "id": "Y5YX-JnVJ1q9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dokaz:\n",
    "$$\n",
    "\\underset{\\theta}{\\mathrm{min}} \\, \\mathrm{KL}(p_D, p_\\theta) = \\mathrm{Nadopuni}\n",
    "$$\n"
   ],
   "metadata": {
    "id": "wAe65zwZKScr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 2.**: Definicija modela $p_\\theta$."
   ],
   "metadata": {
    "id": "mcltA0gCk94E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naš sljedeći korak je definirati model $p_\\theta$. Upravo po definiciji  $p_\\theta$ razlikujemo različite generativne modele.\n",
    "Na primjer autoregresivna faktorizacija modela dovodi do autoregresijskih modela, dok modeliranje nenormalizirane distribucije dovodi do energijskih modela. U ovoj vježbi se fokusiramo  na vrstu generativnih modela koje nazivamo normalizirajući tokovi. Normalizirajući tok $p_\\theta$ definiramo pomoću formule za zamjenu varijabli distribucije.\n",
    "\n",
    "Pretpostavimo da su $\\underline{\\mathbf{x}}$ i $\\underline{\\mathbf{z}}$ dvije kontinuirane slučajne varijable. Pretpostavimo da bijekcija $f$ mapira realizacije slučajne varijable $\\underline{\\mathbf{x}}$ u realizacije od $\\underline{\\mathbf{z}}$, to jest $\\mathbf{z} = f(\\mathbf{x})$. S obzirom da je $f$ bijekcija vrijedi i obrat $\\mathbf{x} = f^{-1}(\\mathbf{z})$. Formulu za zamjenu varijabi distribucije definiramo kao:\n",
    "$$\n",
    " p(\\mathbf{x}) = q(\\mathbf{z}) \\left| \\mathrm{det} \\frac{\\partial \\mathbf{z}}{ \\partial \\mathbf{x}} \\right|, \\quad \\mathbf{z} = f(\\mathbf{x})\n",
    "$$\n",
    "$\\frac{∂ \\mathbf{z}}{ ∂ \\mathbf{x}}$ predstavlja Jakobijan funkcije $f$ za $\\textbf{x}$. det predstavlja determinantu. $|\\cdot|$ predstavlja apsolutnu vrijednost.\n",
    "To jest, drugi član umnoška odgovara apsolutnoj vrijednosti determinante Jakobijana transformacije $f$."
   ],
   "metadata": {
    "id": "-QTBdTnEMnk2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Formulu za zamjenu varijabli ćemo iskoristiti za izgradnju okvira koji nam omogućava implementaciju dubokog generativnog modela.\n",
    "Ideja je jednostavna, distribuciju $q(\\mathbf{z})$ možemo predefinirati (npr. kao Gaussovu distribuciju). Tada nam je jedina nepoznanica bijekcija $f$ koju možemo parametrizirati s $\\theta$ te naučiti optimizacijskim postupkom.\n"
   ],
   "metadata": {
    "id": "Q1WEnc3ZarRI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prisjetimo se sljedećih teorema:\n",
    "1. kompozicija bijekcija je bijekcija, tj $f$ možemo raspisati kao $f = f_1 ∘ f_2 ∘ f_3 ∘ \\dots \\circ f_K$.\n",
    "2. pravilo ulančavanja: $\\frac{∂ z_3}{ ∂ z_1} = \\frac{∂ z_3}{ ∂ z_2}\\frac{∂ z_2}{ ∂ z_1} $\n",
    "3. determinanta umnoška dviju kvadratnih matrica istog reda jednaka je umnošku determinanti svake matrice (Binet-Cauchyjev teorem)\n",
    "4. Apsolutna vrijednost umnoška dva realna broja je jednaka umnošku apsolutnih vrijednosti: $|a\\cdot b| = |a|\\cdot |b|$\n",
    "\n",
    "Bijekciju $f$ možemo promatrati kao kompoziciju funkcija, gdje svaka funkcija $f_i$ prima varijablu $z_{i-1}$ i računa varijablu $z_i$.\n",
    "Ako uzmemo u obzir prethodno spomenute teoreme, dolazimo do sljedeće formulacije:\n",
    "$$\n",
    " p_\\theta (\\mathbf{x}) = q(\\mathbf{z}_K) \\prod_{k=1}^K \\left| \\mathrm{det} \\frac{\\partial \\mathbf{z}_{k} }{ \\partial \\mathbf{z}_{k-1}} \\right|, \\quad \\mathbf{z}_{k} = f_{\\theta_k}(\\mathbf{z}_{k-1}),\n",
    "$$\n",
    "gdje je $\\mathbf{z}_{0} = \\mathbf{x}$.\n",
    "$q(\\mathbf{z}_K)$ može biti proizvoljna distribucija pa je definiramo kao $q(\\mathbf{z}_K):= \\mathcal{N}(\\mathbf{z}_K; 0, \\mathrm{I})$."
   ],
   "metadata": {
    "id": "OjcJuvHaQOBV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 2.** Krenuvši od općenite formule za zamjenu varijabli izvedite izraz za normalizirajući tok."
   ],
   "metadata": {
    "id": "BVx7UqIZh19J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Izvod:\n",
    "$$\n",
    "p(\\mathbf{x}) = q(\\mathbf{z}_K) \\left| \\mathrm{det} \\frac{\\partial f(\\mathbf{x})}{ \\partial \\mathbf{x}} \\right| \\\\ \\dots\n",
    "$$"
   ],
   "metadata": {
    "id": "QcG4_w3XiVxC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spomenimo još da nas općenito zanima logaritam gustoće vjerojatnosti pa pišemo:\n",
    "$$\n",
    " \\ln p_\\theta (\\mathbf{x}) = \\ln q(\\mathbf{z}_K) +\\sum_{k=1}^K \\ln \\left| \\mathrm{det} \\frac{\\partial \\mathbf{z}_{k} }{ \\partial \\mathbf{z}_{k-1}} \\right|, \\quad \\mathbf{z}_{k} = f_{\\theta_k}(\\mathbf{z}_{k-1}), \\tag{1}\n",
    "$$"
   ],
   "metadata": {
    "id": "GzQTQ_1pBBcF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ostala nam je još jedna nepoznanica, a to je dizajn funkcija $f_k$,  $k=1,...,K$.\n",
    "Dizajn funkcije f ovisi o podacima koje modeliramo. Razmislite kako biste bijekciju modelirali kao gradivni blok dubokog modela."
   ],
   "metadata": {
    "id": "N9gRzeKonoKN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeliranje 2D skupa podataka normalizirajućim tokom"
   ],
   "metadata": {
    "id": "2LJNZ2ez-138"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem 3.** Modeliranje 2D skupa podatka."
   ],
   "metadata": {
    "id": "8pnaIDgmlMKu"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E8oosfEPpUaR",
    "ExecuteTime": {
     "end_time": "2025-01-27T17:35:22.588610Z",
     "start_time": "2025-01-27T17:35:17.619207Z"
    }
   },
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "class GaussianMixture2DDataset:\n",
    "    def __init__(self, num_samples, n_comp=2, loc=torch.zeros(2,2), scale=torch.ones(2,2), pi=(torch.ones(2)/2)):\n",
    "        samples_per_component = (pi * num_samples).long()\n",
    "        self.samples = torch.cat([torch.randn(samples_per_component[i],2) * scale[i] + loc[i] for i in range(n_comp)], 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ],
   "metadata": {
    "id": "FPjc6n7sqLIn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Promotrimo sljedeći skup podataka. Pretpostavimo da nam je dostupan samo skup podataka $D$. Cilj nam je aproksimirati distribuciju podataka $p_D$ pomoću normalizirajućeg toka."
   ],
   "metadata": {
    "id": "qrDsz9ajlmYb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "D = GaussianMixture2DDataset(9999, loc=torch.tensor([[-1, -1], [1,1]]), scale=torch.tensor([[0.5, 0.5], [0.5,0.5]])).samples\n",
    "plt.scatter(D[:, 0], D[:, 1], 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "SdbqYcV0sOZK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "S obzirom da modeliramo 2D skup podatka, zanimaju nas funkcije s domenom $\\mathbb{R}^2$ i kodomenom $\\mathbb{R}^2$ koje imaju inverz.\n",
    "Definiramo sljedeći implementacijski ovir:"
   ],
   "metadata": {
    "id": "e6UCmiYqoTxf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class _Bijection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_Bijection, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def inverse(self, z):\n",
    "        pass"
   ],
   "metadata": {
    "id": "oB_3FyfGsfmF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funkcija `forward` implementira unaprijedni prolaz kroz sloj i na izlazu vraća transformirani ulaz i logaritam apsolutne vrijednosti determinante Jakobijana funkcije. \\\n",
    "Funkcija `inverse` implementira inverzni probalz kroz sloj i na izlazu vraća transformirani ulaz."
   ],
   "metadata": {
    "id": "NLaq4kuYrv21"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 3.**: Pokušajmo ponuditi bijektivnu alternativu sloju torch.nn.Linear za dvodimenzionalne podatke. Obratite pozornost na to da je transformaciju potrebno održavati invertibilnom regularizacijskim članom.\n",
    "Nadopunite forward i inverse funkciju.\n",
    "\n",
    "Uputa: Linearna transformacija se primjenjuje na svaki primjer u minigrupi nezavisno. Zbog malene dimenzionalnosti, invertiranje i računanje determinante Jakobijana možemo izvesti grubom silom, izravnim pozivom odgovarajućih funkcija torcha."
   ],
   "metadata": {
    "id": "VIFMGWxqri3u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sum_except_batch(x, num_dims=1):\n",
    "    return x.reshape(*x.shape[:num_dims], -1).sum(-1)\n",
    "\n",
    "class BijectiveLinear2D(_Bijection):\n",
    "    def __init__(self, dim):\n",
    "        super(_Bijection, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.eye(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, dim))\n",
    "\n",
    "    def forward(self, x):   # x has shape NxD\n",
    "        # ...\n",
    "        return z, log_abs_det # shapes NxD, N\n",
    "\n",
    "    def inverse(self, z):\n",
    "        # ...\n",
    "        return x    # NxD\n",
    "\n",
    "    def regularization(self):\n",
    "        return ((self.weight @ self.weight) - torch.eye(self.dim)).abs().sum()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "lS9FtpcPqVpJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 4.**: Nadopunjavanje izvedbe osnovnog razreda `NormalizingFlow`.\n",
    "\n",
    "Proučite predloženu nepotpunu izvedbu razreda `NormalizingFlow`. Obratite pozornost na to da konstruktor razreda prima argument `transforms` koji mora biti tipa [torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) kako bismo kroz taj objekt mogli iterirati pri unaprijednom i inverznom prolazu. Iteraciju ćemo provoditi `for` petljom i tako prolaziti kroz slojeve normalizirajućeg toka. Predložite potpunu izvedbu razreda `NormalizingFlow` pod pretpostavkom da svi elementi slijednog modela `transforms` nasljeđuju prethodno definirano sučelje `_Bijection`. Vaše rješenje može dodavati nove lokalne metode razredu `NormalizingFlow`.\n",
    "\n"
   ],
   "metadata": {
    "id": "8aKtTZhIr7tN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for normalizing flow.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms, input_dim):\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.transforms = transforms # has to be of type nn.Sequential.\n",
    "\n",
    "        self.register_buffer('loc', torch.zeros(input_dim))\n",
    "        self.register_buffer('log_scale', torch.zeros(input_dim))\n",
    "        self.base_dist = torch.distributions.Normal(self.loc, torch.exp(self.log_scale))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Transforms the input sample to the latent representation z.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input sample\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: latent representation of the input sample\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        return z\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\"Transforms the latent representation z back to the input space.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): latent representation\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: representation in the input space\n",
    "        \"\"\"\n",
    "        # ...\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Calculates the log-likelihood of the given sample x (see equation (1)).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: log-likelihood of x\n",
    "        \"\"\"\n",
    "        z = # ...\n",
    "        log_abs_det = # ...\n",
    "        log_pz = # ...\n",
    "        log_px =  log_pz + log_abs_det\n",
    "        return log_px\n",
    "\n",
    "    def sample(self, num_samples, T=1):\n",
    "        \"\"\"Generates new samples from the normalizing flow.\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): number of samples to generate\n",
    "            T (float, optional): sampleing temperature. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: generated samples\n",
    "        \"\"\"\n",
    "        z = z_dist.sample(torch.Size([num_samples])) * T\n",
    "        x = #...\n",
    "        return x"
   ],
   "metadata": {
    "id": "SNBd58SdJBkn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "U jednostavnom slučaju, transformaciju normalizirajućeg toka možemo definirati kao slijed linearnih transformacija:"
   ],
   "metadata": {
    "id": "K4jw_hqiZDVI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleNF(NormalizingFlow):\n",
    "    def __init__(self, input_dim, num_steps=2):\n",
    "        transforms = nn.Sequential()\n",
    "        for _ in range(num_steps):\n",
    "            transforms.append(BijectiveLinear(input_dim))\n",
    "\n",
    "        super(SimpleNF, self).__init__(transforms=transforms, input_dim=input_dim)\n",
    "\n",
    "    def gather_regularization(self):\n",
    "        return sum([m.regularization() for m in self.transforms])"
   ],
   "metadata": {
    "id": "GPTAAO6kr6ei"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Provjerite svoju implementaciju inverza sljedećim kodom:"
   ],
   "metadata": {
    "id": "2S76bp33sZxD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "flow = SimpleNF(2, num_steps=7)\n",
    "print((flow.inverse(flow(D)) - D).sum().item())\n",
    "assert (flow.inverse(flow(D)) - D).sum() < 1e-9"
   ],
   "metadata": {
    "id": "9_YbUH-erqEx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tok učimo optmizacijom prethodno objašnjenog gubitka."
   ],
   "metadata": {
    "id": "yw1rECQCsgrd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "flow = SimpleNF(2, num_steps=7)\n",
    "optim = torch.optim.SGD(flow.parameters(), lr=1e-1)\n",
    "\n",
    "for iter in range(4000):\n",
    "    optim.zero_grad()\n",
    "    log_px = flow.log_prob(D)\n",
    "    loss = - (log_px).mean()\n",
    "    loss_reg = flow.gather_regularization()\n",
    "    total_loss = loss + 0.001 * loss_reg\n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Iter {iter+1}: Loss:{loss.item()} Reg:{loss_reg.item()}\")"
   ],
   "metadata": {
    "id": "m6VldF_vw8tu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ako ste do sada sve napravili kako treba, predloženi postav trebao bi uspjeti naučiti tok s gubitkom od oko 2.5.\n",
    "\n",
    "Provjerimo još jednom stabilnost inverza. Kako biste komentrali rezultate?"
   ],
   "metadata": {
    "id": "OPKOed7X9lP4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print((flow.inverse(flow(D)) - D).sum())\n",
    "assert (flow.inverse(flow(D)) - D).sum() < 1e-4"
   ],
   "metadata": {
    "id": "c8j5If0Y27kW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizirajućim tokom možemo generirati primjere na sljedeći način:"
   ],
   "metadata": {
    "id": "RRpZANT_9uyT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_ = flow.sample(2000).detach()\n",
    "plt.scatter(X_[:, 0], X_[:, 1], 1)\n",
    "plt.title(\"Generated samples\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "-nC0KnXQ_6Zo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.clf()\n",
    "plt.scatter(D[:, 0], D[:, 1], 1)\n",
    "plt.scatter(X_[:, 0], X_[:, 1], 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "O7ZL0ILmAUJ4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tok definiran razredom SimpleNF ne može ostvariti veliki kapacitet jer je kompozicija linearnih transformacija i dalje linearna transformacija. Stoga modelirana distribucija ne uspijeva dovoljno dobro aproksimirati $p_D$. Bolju aproksimaciju dobiti ćemo sofisticiranijom arhitekturom pod nazivom [RealNVP](https://arxiv.org/abs/1605.08803). RealNVP se sastoji od \"slojeva\" afinog miješanja koji su invertibilni po konstrukciji. Imajte na umu da afino miješanje tipično ostvarujemo s više slojeva dubokog modela jer modul miješanja `net` može sadržavati (i tipično sadrži) veći broj uzastopnih nelinearnih transformacija.\n",
    "\n",
    "Unaprijedni prolaz kroz afino miješanje s ulazom $\\mathbf{x}$ dimenzionalnosti $d$ definiramo kao ($m < d$):\n",
    "$$\n",
    "\\mathbf{z}_{:m} = \\mathbf{x}_{:m}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{z}_{m:d} = \\exp(\\ln\\mathbf{s}) \\odot \\mathbf{x}_{m:d} + \\mathbf{t}, \\quad \\ln s, t = net(\\mathbf{x}_{:m})\n",
    "$$"
   ],
   "metadata": {
    "id": "SQcdNm-wuVTR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 5.**: Skicirajte računske grafove unaprijednog i unatražnog prolaza kroz afino miješanje. \\\n",
    "Nadopunite predloženi razred `AffineCouplingLayer` s prikladnom funkcionalnošću.\n",
    "Afino miješanje transformira samo polovicu ulaznog tezora $\\mathbf{x}_{m:d}$, dok druga polovica $\\mathbf{x}_{:m}$ ostaje nepromijenjena.\n",
    "Modul miješanja `net` predviđa parametre miješanja `log_s` i `t` iz polovice ulaznog tenzora $\\mathbf{x}_{:m}$."
   ],
   "metadata": {
    "id": "muwz51aNwVdb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AffineCouplingLayer(_Bijection):\n",
    "    def __init__(self, net):\n",
    "        super(AffineCouplingLayer, self).__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x): # NxD\n",
    "        # ...\n",
    "        return z, log_det # NxD , N\n",
    "\n",
    "    def inverse(self, y): # NxD\n",
    "        # ...\n",
    "        return x # NxD"
   ],
   "metadata": {
    "id": "zCh0mGhOeK1Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 6.:** Zamislimo situaciju gdje slijedno primjenjujemo više coupling slojeva. Možemo primijetiti da će se miješati samo jedna polovica tenzora. Kako bismo osigurali da se miješaju obje polovice ulaznog tenzora, uvodimo transformaciju `SwitchSides` koju ćemo primijeniti nakon svakog coupling sloja.\n",
    "Na taj način osiguravamo da miješanje provodimo naizmjenično na dvjema polovicama ulaznog tenzora. \\\n",
    "Tenzor dijelimo po dimenziji značajki. Dovršite implementaciju razreda `SwitchSides` tako da dijeli ulazni tenzor na dva dijela po dimenziji značajki i zamjenjuje im strane. Možete iskoristiti funkcije [chunk](https://pytorch.org/docs/stable/generated/torch.chunk.html) i [cat](https://pytorch.org/docs/stable/generated/torch.cat.html)."
   ],
   "metadata": {
    "id": "5O75bubJeUNM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SwitchSides(_Bijection):\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ...\n",
    "        return y, 0.\n",
    "\n",
    "    def inverse(self, z):\n",
    "        # ...\n",
    "        return x"
   ],
   "metadata": {
    "id": "nAGiRWSqgNAl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 7.:**U nastavku je dan razred `SimpleTransform` kojeg ćemo koristiti kao modul afinog miješanja u coupling sloju. Primijetite da će primjerci tog razreda biti atributi `net` razreda `AffineCouplingLayer`.\\\n",
    "Dovršite implementaciju tako da `model` bude potpuno povezani modul sljedeće arhitekture:\n",
    "\n",
    "`fc(dim, internal_dim) -> relu -> fc(internal_dim, internal_dim) -> relu -> fc(internal_dim, 2*dim)`.\n",
    "\n",
    "Dimenziju skrivenog sloja potpuno povezanog modula određujemo parametrom `inflate_coef`. \\\n",
    "\n",
    "\n",
    "Inicijalizirajte parametre modula miješanja tako da modul provodi transformaciju koja je jednaka identitetu. \\"
   ],
   "metadata": {
    "id": "8sqlpbo-gi19"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleTransform(nn.Module):\n",
    "    def __init__(self, dim, inflate_coef=1):\n",
    "        super(SimpleTransform, self).__init__()\n",
    "        self.dim = dim\n",
    "        internal_dim = int(dim * inflate_coef)\n",
    "        self.model = nn.Sequential(\n",
    "            # ...\n",
    "        )\n",
    "        # initialize self.model to identity\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        log_s, t = torch.chunk(out, dim=1, chunks=2)\n",
    "        return log_s, t"
   ],
   "metadata": {
    "id": "sLGHf7WPgiIr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleRealNVP(NormalizingFlow):\n",
    "    def __init__(self, input_dim, num_steps=2):\n",
    "        transforms = nn.Sequential()\n",
    "        for i in range(num_steps):\n",
    "            transforms.append(AffineCouplingLayer(SimpleTransform(input_dim//2)))\n",
    "            if i != num_steps - 1:\n",
    "                transforms.append(SwitchSides())\n",
    "        super(SimpleRealNVP, self).__init__(transforms, input_dim)\n"
   ],
   "metadata": {
    "id": "BB25jCUlhIly"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tok možemo naučiti na način kako slijedi:"
   ],
   "metadata": {
    "id": "7CsDfjxsh_qC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "flow = SimpleRealNVP(2, num_steps=7)\n",
    "optim = torch.optim.SGD(flow.parameters(), lr=1e-2, nesterov=True, momentum=0.9)\n",
    "\n",
    "for iter in range(2000):\n",
    "    optim.zero_grad()\n",
    "    log_px = flow.log_prob(D)\n",
    "    loss = - (log_px).mean()\n",
    "    total_loss = loss\n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Iter {iter+1}: Loss:{loss.item()}\")"
   ],
   "metadata": {
    "id": "MrmFJy6FJFDt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print((flow.inverse(flow(D)) - D).sum())\n",
    "assert (flow.inverse(flow(D)) - D).sum() < 1e-4"
   ],
   "metadata": {
    "id": "l_wcddTYw64q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_ = flow.sample(2000, 1).detach()\n",
    "plt.scatter(X_[:, 0], X_[:, 1], 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "jDy--jYaJO2Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.clf()\n",
    "plt.scatter(D[:, 0], D[:, 1], 1)\n",
    "plt.scatter(X_[:, 0], X_[:, 1], 1)"
   ],
   "metadata": {
    "id": "5TIb8DhgJpB1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kao što vidite, kapacitetniji tok uspijeva dobro aproksimirati dani skup podataka."
   ],
   "metadata": {
    "id": "YXCecEFb-ae8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeliranje slika normalizirajućim tokom"
   ],
   "metadata": {
    "id": "cntMrCuDwlkj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naš sljedeći zadatak je modelirati distribuciju slika s normalizirajućim tokom. Slike možemo prestaviti diskretnim tenzorom $\\mathbf{x} \\in [0, 255]^{C \\times H \\times W}$. Prisjetimo se da smo normalizirajući tok definirali za kontinuirane slučajne varijable. Stoga, potrebno je transfrormirati skup podataka u kontinuiranu domenu pomoću zašumljivanja:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{x} + \\mathbf{u}, \\quad \\mathbf{u} \\sim \\mathrm{U}[0, 1)\n",
    "$$\n",
    "U prestavlja uniformnu razdiobu. Sada, naš model uči modelirati zašumljenu sliku. Više o dekvantizaciji zašumljivanjem možete pronaći [ovdje](https://arxiv.org/pdf/1511.01844.pdf).\n",
    "\n",
    "MNIST je skup jednostavnih slika pa ćemo se sljedećim pojednostavljenjem. Sliku ćemo ispeglati u vektor i normalizirajućim tokom učiti takvu reprezentaciju slike.\n",
    "\n",
    "U nastavku je dan kod za pripremanje skupa slika."
   ],
   "metadata": {
    "id": "fihSoOBAfgWD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as tf\n",
    "from torch.utils.data import DataLoader, Subset"
   ],
   "metadata": {
    "id": "mjAxWg880ZoT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_ds = MNIST('.', train=True, download=True, transform=tf.Compose([tf.Resize(14), tf.ToTensor()]))\n",
    "test_ds = MNIST('.', train=False, download=True, transform=tf.Compose([tf.Resize(14), tf.ToTensor()]))\n",
    "\n",
    "selected_number = 0\n",
    "\n",
    "indices = [r[0] for r in list(filter(lambda x: x[1] == selected_number ,[(i, train_ds[i][1]) for i in range(len(train_ds))]))]\n",
    "train_ds = Subset(train_ds, indices)\n",
    "\n",
    "indices = [r[0] for r in list(filter(lambda x: x[1] == selected_number ,[(i, test_ds[i][1]) for i in range(len(test_ds))]))]\n",
    "test_ds = Subset(test_ds, indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "id": "hlg0BSAw0bGb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_mnist(samples, num_row, num_col):\n",
    "    assert len(samples) ==  num_row * num_col\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(len(samples)):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "MBQdhaR2o1y4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "samples = [train_ds[i][0][0] for i in range(10)]\n",
    "plot_mnist(samples, 2, 5)"
   ],
   "metadata": {
    "id": "8plOmcG3o7O6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 8.:** Predložite arhitekturu toka koji će moći naučiti distribuciju vektoriziranih slika. Predlažemo koristiti rezidualni blok sljedeće arhitekture.\n",
    "\n",
    "`fc(dim, dim) -> BN -> relu -> fc(dim, dim) -> relu`\n",
    "\n",
    "Potrebno je dodati rezidualnu vezu od ulaza u rezidualni blok do latentne reprezentacije prije posljednje zglobnice.\n",
    "Dovršite implementaciju i validirajte utjecaj batchnorma na proces učenja i konačne rezultate."
   ],
   "metadata": {
    "id": "5xkU9mPWkiwy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, use_bn=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layers = nn.Sequential()\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.layers(x)\n",
    "        out = out + identity\n",
    "        return self.f_relu(out)"
   ],
   "metadata": {
    "id": "5RJt87gc9HoI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 9.:** Po uzoru na razred `SimpleTransform` dovršite implementaciju razreda `SimpleResidualTransform` čiji se modul miješanja sastoji od rezidualnih blokova. Predlažemo sljedeću arhitekturu:\n",
    "\n",
    "`fc(dim, internal_dim) -> relu -> res_block(internal_dim, internal_dim) -> fc(internal_dim, 2*dim)`\n",
    "\n",
    "Eksperimentirajte s brojem rezidualnih blokova. Kako broj rezidualnih blokova utječe na rezultate?"
   ],
   "metadata": {
    "id": "zaPosHKakhA8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleResidualTransform(nn.Module):\n",
    "    def __init__(self, dim, inflate_coef=1):\n",
    "        super(SimpleResidualTransform, self).__init__()\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ..."
   ],
   "metadata": {
    "id": "Cdj5_SS1kS1B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zadatak 10.:** Dovršte implementaciju razreda `RealNVP` po uzoru na `SimpleRealNVP`. `RealNVP` treba koristiti `SimpleResidualTransform` umjesto `SimpleTransform` kao modul miješanja."
   ],
   "metadata": {
    "id": "378krGZ-zZUh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RealNVP(NormalizingFlow):\n",
    "    def __init__(self, input_dim, num_steps=2):\n",
    "        transforms = nn.Sequential()\n",
    "        # ... define transforms\n",
    "        super(RealNVP, self).__init__(transforms, input_dim)"
   ],
   "metadata": {
    "id": "4wi9loJekhwN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "U nastavku je kod za učenje toka.\n",
    "\n",
    "Obratite pozornost na to da izglednost slike prikazujemo u bitovima po dimenziji (BPD). Podsjetite se što ta mjera predstavlja i kako ju računamo."
   ],
   "metadata": {
    "id": "uPJ_jdPGz7F9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda'\n",
    "flow = RealNVP(14*14, num_steps=21).to(device)\n",
    "optim = torch.optim.AdamW(flow.parameters(), lr=1e-5)\n",
    "epochs = 1000\n",
    "bits = 8\n",
    "for ep in range(epochs):\n",
    "    total_loss = 0.\n",
    "    iters = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = (x * 255).long()\n",
    "        u = torch.rand_like(x.float())\n",
    "        x = x + u\n",
    "        x = x / (2 ** bits)\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        log_px = flow.log_prob(x.to(device)) - np.log((2 ** bits)) * (14 * 14)\n",
    "        loss = - (log_px).mean()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        iters += 1\n",
    "        total_loss += loss.item()\n",
    "    if ep % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_iters = 0\n",
    "            total_bpd = 0.\n",
    "            for x, _ in test_loader:\n",
    "                x = (x * 255).long()\n",
    "                u = torch.rand_like(x.float())\n",
    "                x = x + u\n",
    "                x = x / (2 ** bits)\n",
    "                x = x.flatten(start_dim=1)\n",
    "                log_px = flow.log_prob(x.to(device)) - np.log((2 ** bits)) * (14 * 14)\n",
    "                bpd = -(log_px).mean().cpu() / (np.log(2) * (14 * 14))\n",
    "                total_bpd += bpd\n",
    "                val_iters += 1\n",
    "        samples = flow.sample(10).detach().cpu()\n",
    "        samples = torch.unflatten(samples, dim=1, sizes=(14, 14)).unsqueeze(1)\n",
    "\n",
    "        plot_mnist(samples[:, 0], 2, 5)\n",
    "\n",
    "        print(f\"Epoch {ep+1}: Train Loss:{total_loss / iters} Val BPD: {total_bpd / val_iters}\")"
   ],
   "metadata": {
    "id": "X9Uiqr_E0XOj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "samples = flow.sample(10).detach().cpu()\n",
    "samples = torch.unflatten(samples, dim=1, sizes=(14, 14)).unsqueeze(1)\n",
    "plot_mnist(samples[:,0], 2, 5)"
   ],
   "metadata": {
    "id": "DQ6XLhRHjGp3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus zadatak\n",
    "\n",
    "Predložite implementaciju konvolucijskog normalizirajućeg toka."
   ],
   "metadata": {
    "id": "VP_LQwbL-qY7"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-kbuDupRL5rA"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
